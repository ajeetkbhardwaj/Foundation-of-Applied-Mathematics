{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Contents :</p> <ol> <li>Introduction to Numerical Linear Algebra</li> <li>Direct Methods</li> <li>Condition and Stability</li> <li>Eigenvalues Solving</li> <li>Iterative Methods</li> <li>Miscellaneous Topics</li> </ol>"},{"location":"Eigenvalue/","title":"Eigenvalue Problems","text":"<p>Introduction to Eigenvalue Problems</p> <p>QR Factorization</p> <p>QR Algorithm</p> <p>Divide and Conquer</p> <p>SVD</p> <p><code>Quick review</code>: Gram-Schmidt Process</p> <p>If a vector group \\(\\boldsymbol{\\alpha }_1,\\boldsymbol{\\alpha }_2,\\cdots ,\\boldsymbol{\\alpha }_k\\) is linearly independent, let:</p> \\[ \\boldsymbol{\\beta }_1=\\boldsymbol{\\alpha }_1, \\] \\[ \\boldsymbol{\\beta }_2=\\boldsymbol{\\alpha }_2-\\frac{\\left( \\boldsymbol{\\alpha }_2,\\boldsymbol{\\beta }_1 \\right)}{\\left( \\boldsymbol{\\beta }_1,\\boldsymbol{\\beta }_1 \\right)}\\boldsymbol{\\beta }_1, \\] \\[ \\boldsymbol{\\beta }_3=\\boldsymbol{\\alpha }_3-\\frac{\\left( \\boldsymbol{\\alpha }_3,\\boldsymbol{\\beta }_1 \\right)}{\\left( \\boldsymbol{\\beta }_1,\\boldsymbol{\\beta }_1 \\right)}\\boldsymbol{\\beta }_1-\\frac{\\left( \\boldsymbol{\\alpha }_2,\\boldsymbol{\\beta }_2 \\right)}{\\left( \\boldsymbol{\\beta }_2,\\boldsymbol{\\beta }_2 \\right)}\\boldsymbol{\\beta }_2, \\] \\[ \\vdots \\] \\[ \\boldsymbol{\\beta }_k=\\boldsymbol{\\alpha }_k-\\frac{\\left( \\boldsymbol{\\alpha }_k,\\boldsymbol{\\beta }_1 \\right)}{\\left( \\boldsymbol{\\beta }_1,\\boldsymbol{\\beta }_1 \\right)}\\boldsymbol{\\beta }_1-\\frac{\\left( \\boldsymbol{\\alpha }_k,\\boldsymbol{\\beta }_2 \\right)}{\\left( \\boldsymbol{\\beta }_2,\\boldsymbol{\\beta }_2 \\right)}\\boldsymbol{\\beta }_2-\\cdots -\\frac{\\left( \\boldsymbol{\\alpha }_k-\\boldsymbol{\\beta }_{k-1} \\right)}{\\left( \\boldsymbol{\\beta }_{k-1},\\boldsymbol{\\beta }_{k-1} \\right)}\\boldsymbol{\\beta }_{k-1} \\] <p>Then \\(\\boldsymbol{\\beta }_1,\\boldsymbol{\\beta }_2,\\cdots ,\\boldsymbol{\\beta }_k\\) are orthogonal to each other. Unitize them, we get:</p> \\[ \\boldsymbol{\\gamma }_1=\\frac{\\boldsymbol{\\beta }_1}{\\left\\| \\boldsymbol{\\beta }_1 \\right\\|},\\boldsymbol{\\gamma }_2=\\frac{\\boldsymbol{\\beta }_2}{\\left\\| \\boldsymbol{\\beta }_2 \\right\\|},\\cdots ,\\boldsymbol{\\gamma }_k=\\frac{\\boldsymbol{\\beta }_k}{\\left\\| \\boldsymbol{\\beta }_k \\right\\|} \\] <p>The process from \\(\\boldsymbol{\\alpha }_1,\\boldsymbol{\\alpha }_2,\\cdots ,\\boldsymbol{\\alpha }_k\\) to \\(\\boldsymbol{\\gamma }_1,\\boldsymbol{\\gamma }_2,\\cdots ,\\boldsymbol{\\gamma }_k\\) is called Gram-Schmidt Orthogonalization. We can also get:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\boldsymbol{\\alpha }_1&amp;     \\boldsymbol{\\alpha }_2&amp;     \\cdots&amp;     \\boldsymbol{\\alpha }_k\\\\ \\end{matrix} \\right] \\] \\[ =\\left[ \\begin{matrix}     \\boldsymbol{\\gamma }_1&amp;     \\boldsymbol{\\gamma }_2&amp;     \\cdots&amp;     \\boldsymbol{\\gamma }_k\\\\ \\end{matrix} \\right]\\cdot \\left[ \\begin{matrix}     \\left\\| \\boldsymbol{\\beta }_1 \\right\\|&amp;     \\left( \\boldsymbol{\\alpha }_2,\\boldsymbol{\\gamma }_1 \\right)&amp;       \\cdots&amp;     \\left( \\boldsymbol{\\alpha }_k,\\boldsymbol{\\gamma }_1 \\right)\\\\     0&amp;      \\left\\| \\boldsymbol{\\beta }_2 \\right\\|&amp;     \\cdots&amp;     \\left( \\boldsymbol{\\alpha }_k,\\boldsymbol{\\gamma }_2 \\right)\\\\     \\vdots&amp;     \\ddots&amp;     \\ddots&amp;     \\vdots\\\\     0&amp;      \\cdots&amp;     0&amp;      \\left\\| \\boldsymbol{\\beta }_k \\right\\|\\\\ \\end{matrix} \\right] \\]"},{"location":"Eigenvalue/Divide_and_Conquer/","title":"Divide and Conquer","text":"<p>Assume \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\) is SPD. After phase 1, \\(\\boldsymbol{T}=\\left( \\hat{\\boldsymbol{Q}}^{\\left( 0 \\right)} \\right) ^T\\boldsymbol{A}\\hat{\\boldsymbol{Q}}^{\\left( 0 \\right)}\\) where \\(\\boldsymbol{T}\\) is tridiagonal ( \\(\\boldsymbol{T}\\) is also SPD ).</p> <p><code>Main Idea</code>: Split the matrix into smaller matrices in size, and find the eigenvalues in a recursive manner (surgical strategy).</p> <p>Let:</p> \\[ \\boldsymbol{T}=\\left[ \\begin{matrix}     \\boldsymbol{T}_1&amp;       &amp;       &amp;       \\\\     &amp;       &amp;       \\beta&amp;      \\\\     &amp;       \\beta&amp;      &amp;       \\\\     &amp;       &amp;       &amp;       \\boldsymbol{T}_2\\\\ \\end{matrix} \\right]  \\] <p>where \\(\\boldsymbol{T}_1\\in \\mathbb{R} ^{n\\times n},\\boldsymbol{T}_2\\in \\mathbb{R} ^{\\left( m-n \\right) \\times \\left( m-n \\right)}\\). Usually select \\(n\\sim \\frac{m}{2}\\). We either take \\(n=\\lfloor \\frac{m}{2} \\rfloor\\) or \\(n=\\lceil \\frac{m}{2} \\rceil\\).</p> <p>Let \\(\\beta =t_{n+1,n}=t_{n,n+1}\\). We can rewrite \\(\\boldsymbol{T}\\) as:</p> \\[ \\boldsymbol{T}=\\left[ \\begin{matrix}     \\hat{\\boldsymbol{T}}_1&amp;     \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\hat{\\boldsymbol{T}}_2\\\\ \\end{matrix} \\right] +\\left[ \\begin{matrix}     \\boldsymbol{O}&amp;     &amp;       &amp;       \\boldsymbol{O}\\\\     &amp;       \\beta&amp;      \\beta&amp;      \\\\     &amp;       \\beta&amp;      \\beta&amp;      \\\\     \\boldsymbol{O}&amp;     &amp;       &amp;       \\boldsymbol{O}\\\\ \\end{matrix} \\right]  \\] \\[ \\left( \\hat{\\boldsymbol{T}}_1 \\right) _{nn}=\\left( \\boldsymbol{T}_1 \\right) _{nn}-\\beta ; \\\\ \\left( \\hat{\\boldsymbol{T}}_2 \\right) _{11}=\\left( \\boldsymbol{T}_2 \\right) _{11}-\\beta  \\] <p>We can get:</p> \\[ \\boldsymbol{T}=\\hat{\\boldsymbol{T}}+\\left[ \\begin{array}{c}     0\\\\     \\vdots\\\\     0\\\\     \\sqrt{\\beta}\\\\     \\sqrt{\\beta}\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right] \\cdot \\left[ \\begin{matrix}     0&amp;      \\cdots&amp;     0&amp;      \\sqrt{\\beta}&amp;       \\sqrt{\\beta}&amp;       0&amp;      \\cdots&amp;     0\\\\ \\end{matrix} \\right] \\triangleq \\hat{\\boldsymbol{T}}+\\boldsymbol{z}\\cdot \\boldsymbol{z}^T \\] <p>Therefore: \\(\\boldsymbol{T}=\\hat{\\boldsymbol{T}}+\\boldsymbol{z}\\cdot \\boldsymbol{z}^T\\).</p> <p>Question: Assume that we know the eigenvalues of \\(\\hat{\\boldsymbol{T}}_1\\) and \\(\\hat{\\boldsymbol{T}}_2\\), how to find the eigenvalues of \\(\\boldsymbol{T}\\) ?</p> <p>Denote \\(\\hat{\\boldsymbol{T}}_1=\\boldsymbol{Q}_1\\boldsymbol{D}_1{\\boldsymbol{Q}_1}^T\\) as the eigenvalue decomposition for \\(\\hat{\\boldsymbol{T}}_1\\). \\(\\boldsymbol{Q}_1\\) contains the eigenvectors of \\(\\hat{\\boldsymbol{T}}_1\\). \\(\\boldsymbol{D}_1\\) is a diagonal matrix whose diagonal elements are the eigenvalues. Also \\(\\hat{\\boldsymbol{T}}_2=\\boldsymbol{Q}_2\\boldsymbol{D}_2{\\boldsymbol{Q}_2}^T\\).</p> <p>Let us denote the last row of \\(\\boldsymbol{Q}_1\\) as \\({\\boldsymbol{q}_1}^T\\), and the first row of \\(\\boldsymbol{Q}_2\\) as \\({\\boldsymbol{q}_2}^T\\). Also denote \\(\\boldsymbol{v}=\\left[ {\\boldsymbol{q}_1}^T,{\\boldsymbol{q}_2}^T \\right] ^T\\).</p> <p><code>Theorem</code>: \\(\\boldsymbol{T}\\) is similar to \\(\\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] +\\beta \\boldsymbol{vv}^T\\).</p> <p><code>Proof</code>: Define \\(\\boldsymbol{Q}=\\left[ \\begin{matrix}     \\boldsymbol{Q}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{Q}_2\\\\ \\end{matrix} \\right]\\)</p> \\[ \\boldsymbol{Q}\\left( \\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] +\\beta \\boldsymbol{vv}^T \\right) \\boldsymbol{Q}^T \\] \\[ =\\left[ \\begin{matrix}     \\boldsymbol{Q}_1\\boldsymbol{D}_1{\\boldsymbol{Q}_1}^T&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{Q}_2\\boldsymbol{D}_2{\\boldsymbol{Q}_2}^T\\\\ \\end{matrix} \\right] +\\beta \\left[ \\begin{matrix}     \\boldsymbol{Q}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{Q}_2\\\\ \\end{matrix} \\right] \\left[ \\begin{array}{c}     \\boldsymbol{q}_1\\\\     \\boldsymbol{q}_2\\\\ \\end{array} \\right] \\left[ \\begin{matrix}     {\\boldsymbol{q}_1}^T&amp;       {\\boldsymbol{q}_2}^T\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix}     {\\boldsymbol{Q}_1}^T&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     {\\boldsymbol{Q}_2}^T\\\\ \\end{matrix} \\right]  \\] \\[ =\\left[ \\begin{matrix}     \\hat{\\boldsymbol{T}}_1&amp;     \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\hat{\\boldsymbol{T}}_2\\\\ \\end{matrix} \\right] +\\beta \\left[ \\begin{array}{c}     \\boldsymbol{Q}_1\\boldsymbol{q}_1\\\\     \\boldsymbol{Q}_2\\boldsymbol{q}_2\\\\ \\end{array} \\right] \\left[ \\begin{matrix}     {\\boldsymbol{q}_1}^T{\\boldsymbol{Q}_1}^T&amp;       {\\boldsymbol{q}_2}^T{\\boldsymbol{Q}_2}^T\\\\ \\end{matrix} \\right]  \\] \\[ =\\left[ \\begin{matrix}     \\hat{\\boldsymbol{T}}_1&amp;     \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\hat{\\boldsymbol{T}}_2\\\\ \\end{matrix} \\right] +\\beta \\left[ \\begin{array}{c}     0\\\\     \\vdots\\\\     0\\\\     1\\\\     1\\\\     0\\\\     \\vdots\\\\     0\\\\ \\end{array} \\right] \\cdot \\left[ \\begin{matrix}     0&amp;      \\cdots&amp;     0&amp;      1&amp;      1&amp;      0&amp;      \\cdots&amp;     0\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\hat{\\boldsymbol{T}}_1&amp;     \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\hat{\\boldsymbol{T}}_2\\\\ \\end{matrix} \\right] +\\boldsymbol{zz}^T \\] <p>which is \\(\\boldsymbol{T}\\).</p> <p>Question: What are the eigenvalues for \\(\\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] +\\beta \\boldsymbol{vv}^T\\) ?</p> <p>Define \\(\\boldsymbol{w}=\\sqrt{\\beta}\\boldsymbol{v}\\), then \\(\\beta \\boldsymbol{vv}^T=\\boldsymbol{ww}^T\\). What are the eigenvalues for \\(\\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] +\\boldsymbol{ww}^T\\) ?</p> <p><code>Claim</code>: \\(\\boldsymbol{w}\\ne 0\\). This is because \\(\\boldsymbol{v}=\\left[ \\begin{array}{c}     \\boldsymbol{q}_1\\\\     \\boldsymbol{q}_2\\\\ \\end{array} \\right]\\) where \\(\\boldsymbol{q}_1\\) and \\(\\boldsymbol{q}_2\\) are eigenvectors corresponding to \\(\\hat{\\boldsymbol{T}}_1,\\hat{\\boldsymbol{T}}_2\\).</p> <p>We can further assume that \\(w_i\\ne 0,\\left( \\forall i \\right)\\). If not, \\(\\boldsymbol{w}=\\left[ \\begin{array}{c}     w_1\\\\     \\vdots\\\\     w_{i-1}\\\\     0\\\\     w_{i+1}\\\\     \\vdots\\\\     w_m\\\\ \\end{array} \\right]\\) and we can break the problem into two smaller problems (in size) and continue the procedure.</p> <p>Let \\(\\left[ \\begin{matrix}     \\boldsymbol{D}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{D}_2\\\\ \\end{matrix} \\right] \\triangleq \\boldsymbol{D}\\) for simplicity.</p> <p><code>Theorem</code> (important): The eigenvalues of \\(\\boldsymbol{D}+\\boldsymbol{ww}^T\\) are the roots of the following rational function:</p> \\[ f\\left( \\lambda \\right) =1+\\boldsymbol{w}^T\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) ^{-1}\\boldsymbol{w}, \\\\ \\boldsymbol{D}=\\mathrm{diag}\\left( d_1,d_2,\\cdots ,d_m \\right) ; \\] \\[ f\\left( \\lambda \\right) =1+\\sum_{j=1}^m{\\frac{{w_j}^2}{d_j-\\lambda}} \\] <p><code>Proof</code>: If \\(\\left( \\boldsymbol{D}+\\boldsymbol{ww}^T \\right) \\boldsymbol{q}=\\lambda \\boldsymbol{q}\\) for \\(\\boldsymbol{q}\\ne 0\\), we want to show that \\(f(\\lambda )=0\\).</p> <p>Rewrite the equation as \\(\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) \\boldsymbol{q}+\\boldsymbol{w}\\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0\\). Assume \\(\\boldsymbol{D}-\\lambda \\mathbf{I}\\) is invertible (Why can we assume it is invertible?), then:</p> \\[ \\boldsymbol{q}+\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) ^{-1}\\boldsymbol{w}\\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0; \\] \\[ \\boldsymbol{w}^T\\boldsymbol{q}+\\boldsymbol{w}^T\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) ^{-1}\\boldsymbol{w}\\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0 \\] <p>Since \\(\\boldsymbol{w}^T\\boldsymbol{q}\\) is a scalar, then:</p> \\[ \\left( 1+\\boldsymbol{w}^T\\left( \\boldsymbol{D}-\\lambda \\mathbf{I} \\right) ^{-1}\\boldsymbol{w} \\right) \\cdot \\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0; \\\\ \\Longrightarrow f\\left( \\lambda \\right) \\cdot \\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =0 \\] <p>If \\(\\boldsymbol{w}^T\\boldsymbol{q} \\ne 0\\), then we must have \\(f(\\lambda )=0\\); If \\(\\boldsymbol{w}^T\\boldsymbol{q}=0\\), then:</p> \\[ \\lambda \\boldsymbol{q}=\\left( \\boldsymbol{D}+\\boldsymbol{ww}^T \\right) \\boldsymbol{q}=\\boldsymbol{Dq}+\\boldsymbol{w}\\left( \\boldsymbol{w}^T\\boldsymbol{q} \\right) =\\boldsymbol{Dq} \\] <p>This means that \\(\\boldsymbol{q}\\) is an eigenvector of \\(\\boldsymbol{D}\\). \\(\\boldsymbol{D}\\) is diagonal, then \\(\\boldsymbol{q}\\) must be \\(\\boldsymbol{e}_i,1\\leqslant i\\leqslant m\\). \\(0=\\boldsymbol{w}^T\\boldsymbol{q}=\\boldsymbol{w}^T\\boldsymbol{e}_i=w_i\\) and we get the contradiction. This shows that \\(\\boldsymbol{w}^T\\boldsymbol{q}\\ne 0\\Longrightarrow f\\left( \\lambda \\right) =0\\).</p> <p>The converse is also true: If \\(\\lambda\\) is a root of \\(f(\\lambda )=0\\), then \\(\\lambda\\) is an eigenvalue of \\(\\boldsymbol{D}+\\boldsymbol{ww}^T\\).</p> <p>The properties of \\(f(\\lambda )\\): The roots of \\(f(\\lambda )=0\\) can be computed easily by many iterative methods, such as Newton's method because they are well separated and have no flat region.</p> <p></p> <p>We always have root \\({\\lambda}_i\\) in \\((d_i ,d_{i+1})\\):</p> \\[ {\\lambda _i}^{\\left( 0 \\right)}=\\frac{1}{2}\\left( d_i+d_{i+1} \\right) ; \\\\ {\\lambda _i}^{\\left( k+1 \\right)}={\\lambda _i}^{\\left( k \\right)}-\\frac{f\\left( {\\lambda _i}^{\\left( k \\right)} \\right)}{f\\prime\\left( {\\lambda _i}^{\\left( k \\right)} \\right)}; \\] \\[ k\\rightarrow +\\infty : {\\lambda _i}^{\\left( k \\right)}\\rightarrow \\lambda _i \\]"},{"location":"Eigenvalue/Introduction/","title":"Introduction to Eigenvalue Problem","text":"<p><code>Overall Task</code>: Given</p> \\[ \\boldsymbol{Ax}=\\lambda \\boldsymbol{x} \\] <p>We need to find \\(\\lambda\\) and \\(\\boldsymbol{x}\\).</p> <p>What are the methods to compute eigenvalues and eigenvectors?</p>"},{"location":"Eigenvalue/Introduction/#traditional-methods","title":"Traditional Methods","text":""},{"location":"Eigenvalue/Introduction/#traditional-methods-introduction","title":"Traditional Methods Introduction","text":"<p>Traditional Method:</p> <ul> <li>Step 1: Calculate the character polynomial \\(p_{\\boldsymbol{A}}\\left( z \\right) =\\det \\left( z\\boldsymbol{I}-\\boldsymbol{A} \\right)\\);</li> <li>Step 2: Find the roots of \\(p_{\\boldsymbol{A}}\\left( z \\right)\\): \\(\\lambda _i\\). They are eigenvalues;</li> <li>Step 3: Solve \\(\\boldsymbol{Ax}=\\lambda _i\\boldsymbol{x}\\) for \\(\\boldsymbol{x}\\). \\(\\boldsymbol{x}\\) is the eigenvalue corresponding to \\(\\lambda _i\\).</li> </ul>"},{"location":"Eigenvalue/Introduction/#problem-of-traditional-methods","title":"Problem of Traditional Methods","text":"<p><code>Example</code>: Let</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     1&amp;      1\\\\     0&amp;      1\\\\ \\end{matrix} \\right]  \\] <p>Then \\(\\lambda _{1,2}=1\\), \\(p_{\\boldsymbol{A}}\\left( z \\right) =\\left( z-1 \\right) ^2=z^2-2z+1\\). The coefficients encodes in the computer are \\(\\left[ \\begin{matrix}     1&amp;      -2&amp;     1\\\\ \\end{matrix} \\right]\\). Assume that the input is \\(\\left[ \\begin{matrix}     1&amp;      -2&amp;     1-10^{-16}\\\\ \\end{matrix} \\right]\\). Then the polynomial becomes \\(z^2-2z+\\left( 1-10^{-16} \\right) =\\left( z-\\left( 1-10^{-8} \\right) \\right) \\cdot \\left( z-\\left( 1+10^{-8} \\right) \\right) =0\\). We get \\(\\lambda _1=1-10^{-8}, \\lambda _2=1+10^{-8}\\). Therefore, when the input error is \\(10^{-16}\\), the output error is \\(O(10^{-8})\\). The root finding procedure amplifies the error!</p> <p>Root finding is not stable, and algorithms using root finding procedures are not a stable algorithms.</p>"},{"location":"Eigenvalue/Introduction/#root-finding-vs-eigenvalue-problems","title":"Root Finding V.S. Eigenvalue Problems","text":""},{"location":"Eigenvalue/Introduction/#comparison","title":"Comparison","text":"<p>Actually, root finding in computers is implemented as eigenvalue solving algorithms.</p> <p><code>Claim</code>: Eigenvalue problem is equivalent to root finding for polynomials.</p> <p>Given \\(p\\left( z \\right) =z^m+a_{m-1}z^{m-1}+\\cdots +a_1z+a_0\\). Introduce a matrix:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     0&amp;      &amp;       &amp;       O&amp;      -a_0\\\\     1&amp;      \\ddots&amp;     &amp;       &amp;       -a_1\\\\     &amp;       \\ddots&amp;     \\ddots&amp;     &amp;       \\\\     &amp;       &amp;       \\ddots&amp;     0&amp;      -a_{m-2}\\\\     O&amp;      &amp;       &amp;       1&amp;      -a_{m-1}\\\\ \\end{matrix} \\right]  \\] <p>\\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\) is called the companion matrix for \\(p(z)\\). We can prove that:</p> \\[ p_{\\boldsymbol{A}}\\left( z \\right) =\\det \\left( z\\boldsymbol{I}-\\boldsymbol{A} \\right) =p\\left( z \\right)  \\] <p>Then getting the roots for the original polynomial is euqivalent to getting the eigenvalues for the corresponding companion matrix.</p>"},{"location":"Eigenvalue/Introduction/#difficulty","title":"Difficulty","text":"<p>There is fundamental difficulty in the eigenvalue computation: no explicit formula for a general matrix \\(\\boldsymbol{A}_{m\\times m}\\) when \\(m\\geqslant 5\\).</p> <p><code>Theorem</code>: For any \\(m\\geqslant 5\\), there is a polynomial \\(p(z)\\) of degree \\(m\\) with rational coefficients that has a real root \\(p(r)=0\\) with the property that \\(r\\) can NOT be written using any expression involving rational number additions, subtractions, multiplications, divisions or \\(k\\) -th roots.</p> <p>All the methods for eigenvalues must be iterative!</p> <p>Even if working with exact arithmetic, there could be no computing program that would produce the exact roots of an arbitrary polynomial of degree \\(m\\geqslant 5\\) in a finite number of steps. All eigenvalue methods must be iterative.</p>"},{"location":"Eigenvalue/Introduction/#numerical-methods","title":"Numerical Methods","text":"<p>We want to convert the original matrix to a matrix that is easy to recognize the eigenvalues (upper/lower triangular or diagonal), and the transformation should not change the eigenvalues of matrices (this is the concept of similar).</p> <p>Commonly used numerical methods for eigenvalue problem have two phases:</p>"},{"location":"Eigenvalue/Introduction/#phase-one","title":"Phase One","text":"<p>Phase 1: A direct method to produce an upper Hessenberg matrix \\(\\boldsymbol{H}\\) (an upper Hessenberg matrix has zero entries below the first subdiagonal, and a lower Hessenberg matrix has zero entries above the first superdiagonal):</p> \\[ \\boldsymbol{H}=\\left[ \\begin{matrix}     a_{11}&amp;     a_{12}&amp;     a_{13}&amp;     a_{14}&amp;     \\cdots&amp;     \\cdots&amp;     a_{1\\left( n-1 \\right)}&amp;        a_{1n}\\\\     a_{21}&amp;     a_{22}&amp;     a_{23}&amp;     a_{24}&amp;     \\ddots&amp;     \\cdots&amp;     a_{2\\left( n-1 \\right)}&amp;        a_{2n}\\\\     0&amp;      a_{32}&amp;     a_{33}&amp;     a_{34}&amp;     \\ddots&amp;     \\cdots&amp;     a_{3\\left( n-1 \\right)}&amp;        a_{3n}\\\\     0&amp;      0&amp;      a_{43}&amp;     a_{44}&amp;     \\ddots&amp;     \\cdots&amp;     a_{4\\left( n-1 \\right)}&amp;        a_{4n}\\\\     0&amp;      0&amp;      0&amp;      a_{54}&amp;     \\ddots&amp;     \\ddots&amp;     a_{5\\left( n-1 \\right)}&amp;        a_{5n}\\\\     \\vdots&amp;     \\vdots&amp;     \\vdots&amp;     0&amp;      \\ddots&amp;     \\vdots&amp;     \\vdots&amp;     \\vdots\\\\     0&amp;      0&amp;      0&amp;      0&amp;      \\ddots&amp;     a_{\\left( n-1 \\right) \\left( n-2 \\right)}&amp;      a_{\\left( n-1 \\right) \\left( n-1 \\right)}&amp;      a_{\\left( n-1 \\right) n}\\\\     0&amp;      0&amp;      0&amp;      0&amp;      \\cdots&amp;     0&amp;      a_{n\\left( n-1 \\right)}&amp;        a_{nn}\\\\ \\end{matrix} \\right]  \\] <p>The procedure can end within finite number of steps. This is done by similar transforms. In linear algebra, \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) are similar ( \\(\\boldsymbol{A}\\sim \\boldsymbol{B}\\) ), if there exists \\(\\boldsymbol{X}\\in \\mathbb{R} ^{m\\times m}\\) invertible such that \\(\\boldsymbol{XAX}^{-1}=\\boldsymbol{B}\\). We can let:</p> \\[ 0=\\det \\left( \\lambda \\boldsymbol{I}-\\boldsymbol{B} \\right) =\\det \\left( \\lambda \\boldsymbol{I}-\\boldsymbol{XAX}^{-1} \\right) =\\det \\left( \\boldsymbol{X}\\left( \\lambda \\boldsymbol{I}-\\boldsymbol{A} \\right) \\boldsymbol{X}^{-1} \\right) =\\det \\left( \\boldsymbol{X} \\right) \\cdot \\det \\left( \\lambda \\boldsymbol{I}-\\boldsymbol{A} \\right) \\cdot \\det \\left( \\boldsymbol{X}^{-1} \\right) \\] <p>Because \\(\\det \\left( \\boldsymbol{X} \\right) \\ne 0, \\det \\left( \\boldsymbol{X}^{-1} \\right) \\ne 0\\), we know that the characteristic polynomials of \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) are the same, and \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) have the same set of eigenvalues.</p>"},{"location":"Eigenvalue/Introduction/#phase-two","title":"Phase Two","text":"<p>Phase 2: An iterative procedure that is also based on similarity transforms to produce a formally infinite sequence of upper Hessenberg matrices that converges to a triangular form.</p> <p>Why we divide the algorithms into two phases? The cost concern is the reason for the two-phase strategy. Phase 1 has \\(O(m^3)\\) flops, while in Phase 2, it normally converges to \\(O\\left( \\varepsilon _{machine} \\right)\\) within \\(O(m)\\) steps, each step requiring at most \\(O(m^2)\\) steps to finish.</p> <p>Building blocks: QR Factorization.</p>"},{"location":"Eigenvalue/QR_Algorithm/","title":"QR Algorithm","text":"<p>QR Algorithm is the recommended algorithm for eigenvalue and eigenvector computation.</p> <p>The matrix \\(\\boldsymbol{A}\\) below has been computed after phase 1.</p>"},{"location":"Eigenvalue/QR_Algorithm/#pure-qr-algorithm","title":"\"Pure\" QR Algorithm","text":"<p>Pure QR Algorithm:</p> <p>Initialize: \\(\\boldsymbol{A}^{\\left( 0 \\right)}=\\boldsymbol{A}\\);</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>Do QR factorization: \\(\\boldsymbol{Q}^{\\left( k \\right)}\\cdot \\boldsymbol{R}^{\\left( k \\right)}=\\boldsymbol{A}^{\\left( k-1 \\right)}\\);</li> <li>\\(\\boldsymbol{A}^{\\left( k \\right)}=\\boldsymbol{R}^{\\left( k \\right)}\\cdot \\boldsymbol{Q}^{\\left( k \\right)}\\);</li> </ul> <p>End</p>"},{"location":"Eigenvalue/QR_Algorithm/#similarity-transformation","title":"Similarity Transformation","text":"<p>Task 1: Verify that QR algorithm performs similarity transformation, that is, \\(\\boldsymbol{A}^{\\left( k \\right)}\\sim \\boldsymbol{A}^{\\left( k-1 \\right)}\\).</p> <ul> <li>Step 1: \\(\\boldsymbol{A}^{\\left( k-1 \\right)}=\\boldsymbol{Q}^{\\left( k \\right)}\\cdot \\boldsymbol{R}^{\\left( k \\right)}\\Leftrightarrow \\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\boldsymbol{A}^{\\left( k-1 \\right)}=\\boldsymbol{R}^{\\left( k \\right)}\\)</li> <li>Step 2: \\(\\boldsymbol{A}^{\\left( k \\right)}=\\boldsymbol{R}^{\\left( k \\right)}\\cdot \\boldsymbol{Q}^{\\left( k \\right)}=\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\boldsymbol{A}^{\\left( k-1 \\right)}\\boldsymbol{Q}^{\\left( k \\right)}\\Rightarrow \\boldsymbol{A}^{\\left( k \\right)}\\sim \\boldsymbol{A}^{\\left( k-1 \\right)}\\). Then we can get:</li> </ul> \\[ \\boldsymbol{A}^{\\left( k \\right)}=\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\boldsymbol{A}^{\\left( k-1 \\right)}\\boldsymbol{Q}^{\\left( k \\right)}=\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\left( \\boldsymbol{Q}^{\\left( k-1 \\right)} \\right) ^T\\boldsymbol{A}^{\\left( k-2 \\right)}\\boldsymbol{Q}^{\\left( k \\right)}=\\cdots  \\] \\[ =\\underset{\\left( \\bar{\\boldsymbol{Q}}^{\\left( k \\right)} \\right) ^T}{\\underbrace{\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\left( \\boldsymbol{Q}^{\\left( k-1 \\right)} \\right) ^T\\cdots \\left( \\boldsymbol{Q}^{\\left( 1 \\right)} \\right) ^T}}\\cdot \\boldsymbol{A}^{\\left( 0 \\right)}\\cdot \\underset{\\bar{\\boldsymbol{Q}}^{\\left( k \\right)}}{\\underbrace{\\boldsymbol{Q}^{\\left( 1 \\right)}\\cdots \\boldsymbol{Q}^{\\left( k-1 \\right)}\\boldsymbol{Q}^{\\left( k \\right)}}} \\\\ =\\left( \\bar{\\boldsymbol{Q}}^{\\left( k \\right)} \\right) ^T\\boldsymbol{A}\\bar{\\boldsymbol{Q}}^{\\left( k \\right)} \\] <p>If \\(\\boldsymbol{A}^{\\left( k \\right)}\\) converges to \\(\\mathrm{diag}\\left( \\lambda _1,\\cdots ,\\lambda _n \\right)\\) as \\(k\\rightarrow +\\infty\\), then \\(\\bar{\\boldsymbol{Q}}^{\\left( k \\right)}\\rightarrow \\boldsymbol{Q}\\) which contains the eigenvectors of \\(\\boldsymbol{A}\\).</p>"},{"location":"Eigenvalue/QR_Algorithm/#about-phase-one","title":"About Phase One","text":"<p>Task 2: Why do we need phase 1 before the algorithm performs?</p> <p>Let us assume \\(\\boldsymbol{A}\\) is symmetric for simplicity, then \\(\\boldsymbol{H}=\\boldsymbol{QAQ}^T\\) is tridiagonal. Starting from \\(\\boldsymbol{H}\\), the cost for each iteration is of \\(O(m)\\) (using Given Rotation, for example).</p> <p>If we don't perform phase 1, starting from \\(\\boldsymbol{A}\\), the cost for each iteration is \\(O(m^3)\\) (using Householder QR factorization, for example).</p> <p>Question: What is the cost per iteration if \\(\\boldsymbol{H}\\) is and upper Hessenberg matrix? (HW Practice)</p>"},{"location":"Eigenvalue/QR_Algorithm/#convergence","title":"Convergence","text":"<p>Task 3: QR algorithm is convergent.</p> <p><code>Theorem</code>: Let the pure QR algorithm be applied to a real symmetric matrix \\(\\boldsymbol{A}\\) whose eigenvalues satisfy \\(\\left| \\lambda _1 \\right|&gt;\\left| \\lambda _2 \\right|&gt;\\cdots &gt;\\left| \\lambda _m \\right|\\), and the corresponding eigenmatrix \\(\\boldsymbol{Q}\\) has all nonsingular leading principal submatrices. Then as \\(k\\rightarrow +\\infty\\), \\(\\boldsymbol{A}^{\\left( k \\right)}\\) converges linearly with a constant rate given by \\(\\max_j \\frac{\\left| \\lambda _{j+1} \\right|}{\\left| \\lambda _j \\right|}\\) to diagonal matrix \\(\\mathrm{diag}\\left( \\lambda _1,\\lambda _2,\\cdots ,\\lambda _n \\right)\\), and \\(\\bar{\\boldsymbol{Q}}^{\\left( k \\right)}\\) (with signs of its columns adjusted as necessary) converges to \\(\\boldsymbol{Q}\\) at the same rate.</p> <p>In the theorem above, \\(\\boldsymbol{A}=\\boldsymbol{Q}^T\\mathbf{\\Lambda }\\boldsymbol{Q}\\) where \\(\\boldsymbol{Q}\\in \\mathbb{C} ^{m\\times m}\\) is unitary.</p> <p>The sketch of the proof (in three steps):</p>"},{"location":"Eigenvalue/QR_Algorithm/#step-one","title":"Step One","text":"<p>Step 1: QR algorithm is essentially equivalent to the so-called simultaneous iteration.</p> <p>Simultaneous Iteration:</p> <p>Pick \\(\\hat{\\boldsymbol{Q}}^{\\left( 0 \\right)}\\in \\mathbb{R} ^{m\\times n}\\) with orthonormal columns;</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>\\(\\boldsymbol{Z}^{\\left( k \\right)}=\\boldsymbol{A}\\hat{\\boldsymbol{Q}}^{\\left( k-1 \\right)}\\);</li> <li>Do QR factorization: \\(\\hat{\\boldsymbol{Q}}^{\\left( k \\right)}\\hat{\\boldsymbol{R}}^{\\left( k \\right)}=\\boldsymbol{Z}^{\\left( k \\right)}\\);</li> </ul> <p>End</p> <p>Claim: If we pick \\(\\hat{\\boldsymbol{Q}}^{\\left( 0 \\right)}=\\mathbf{I}\\), then the simultaneous iteration becomes the QR algorithm. (How to verify?)</p>"},{"location":"Eigenvalue/QR_Algorithm/#step-two","title":"Step Two","text":"<p>Step 2: Simultaneous iteration is a block Power Iteration.</p> <ul> <li>Power Iteration: \\(\\boldsymbol{Z}^{\\left( k \\right)}=\\boldsymbol{A}\\hat{\\boldsymbol{Q}}^{\\left( k-1 \\right)}\\).</li> <li>Normalization and estimating eigenvalues: \\(\\hat{\\boldsymbol{Q}}^{\\left( k \\right)}\\hat{\\boldsymbol{R}}^{\\left( k \\right)}=\\boldsymbol{Z}^{\\left( k \\right)}\\).</li> </ul>"},{"location":"Eigenvalue/QR_Algorithm/#step-three","title":"Step Three","text":"<p>Step 3: The block Power Iteration is convergent with the rate given by</p> \\[ c=\\max_j \\frac{\\left| \\lambda _{j+1} \\right|}{\\left| \\lambda _j \\right|} \\] <p>if \\(\\left| \\lambda _1 \\right|&gt;\\left| \\lambda _2 \\right|&gt;\\cdots &gt;\\left| \\lambda _j \\right|&gt;\\left| \\lambda _{j+1} \\right|&gt;\\cdots &gt;\\left| \\lambda _m \\right|\\geqslant 0\\). Then we get \\(0&lt;c&lt;1\\).</p> <p>A special case for block Power Iteration to show the convergence (this is the essential idea):</p> <p>Consider \\(\\left[ \\begin{matrix}     {\\boldsymbol{v}_1}^{\\left( 0 \\right)}&amp;      {\\boldsymbol{v}_{\\left( 2 \\right)}}^{\\left( 0 \\right)}\\\\ \\end{matrix} \\right]\\) where \\({\\boldsymbol{v}_1}^{\\left( 0 \\right)}\\) and \\({\\boldsymbol{v}_{\\left( 2 \\right)}}^{\\left( 0 \\right)}\\) are unit vectors and orthogonal to each other. Then:</p> \\[ \\boldsymbol{A}\\left[ \\begin{matrix}     {\\boldsymbol{v}_1}^{\\left( 0 \\right)}&amp;      {\\boldsymbol{v}_{\\left( 2 \\right)}}^{\\left( 0 \\right)}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     {\\boldsymbol{Av}_1}^{\\left( 0 \\right)}&amp;     {\\boldsymbol{Av}_{\\left( 2 \\right)}}^{\\left( 0 \\right)}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{w}_1&amp;       \\boldsymbol{w}_2\\\\ \\end{matrix} \\right]  \\] <p>Also:</p> \\[ \\boldsymbol{QR}=\\left[ \\begin{matrix}     \\boldsymbol{w}_1&amp;       \\boldsymbol{w}_2\\\\ \\end{matrix} \\right] ; \\\\ \\boldsymbol{Q}=\\left[ \\begin{matrix}     {\\boldsymbol{v}_1}^{\\left( 1 \\right)}&amp;      {\\boldsymbol{v}_{\\left( 2 \\right)}}^{\\left( 1 \\right)}\\\\ \\end{matrix} \\right] ;\\cdots  \\] <p>\\(\\boldsymbol{Q}\\) spans the space generated by the eigenvectors \\(\\boldsymbol{q}_1, \\boldsymbol{q}_2\\). We can further prove that \\({\\boldsymbol{v}_1}^{\\left( k \\right)}\\rightarrow \\boldsymbol{q}_1, {\\boldsymbol{v}_2}^{\\left( k \\right)}\\rightarrow \\boldsymbol{q}_2\\) as \\(k\\rightarrow +\\infty\\).</p>"},{"location":"Eigenvalue/QR_Algorithm/#practical-qr-algorithm","title":"\"Practical\" QR Algorithm","text":""},{"location":"Eigenvalue/QR_Algorithm/#algorithm-steps","title":"Algorithm Steps","text":"<p>We assume \\(\\boldsymbol{A}\\) is SPD below.</p> <p>Phase 1 is the same as before: \\(\\left( \\boldsymbol{Q}^{\\left( 0 \\right)} \\right) ^T\\boldsymbol{A}^{\\left( 0 \\right)}\\boldsymbol{Q}^{\\left( 0 \\right)}=\\boldsymbol{A}\\) where \\(\\boldsymbol{A}^{\\left( 0 \\right)}\\) is a tridiagonal matrix;</p> <p>For \\(k=1,2,\\cdots\\):</p> <ul> <li>Pick a shift \\(\\mu ^{\\left( k \\right)}\\);</li> <li>Do QR factorization: \\(\\boldsymbol{Q}^{\\left( k \\right)}\\boldsymbol{R}^{\\left( k \\right)}=\\boldsymbol{A}^{\\left( k \\right)}-\\mu ^{\\left( k \\right)}\\mathbf{I}\\);</li> <li>\\(\\boldsymbol{A}^{\\left( k \\right)}=\\boldsymbol{R}^{\\left( k \\right)}\\boldsymbol{Q}^{\\left( k \\right)}+\\mu ^{\\left( k \\right)}\\mathbf{I}\\);</li> <li>If any off-diagonal element \\({\\boldsymbol{A}_{j,j+1}}^{\\left( k \\right)}\\) is sufficiently small (close to zero is absolute value sense), we set \\({\\boldsymbol{A}_{j,j+1}}^{\\left( k \\right)}={\\boldsymbol{A}_{j+1,j}}^{\\left( k \\right)}=0\\) and obtain: \\(\\boldsymbol{A}^{\\left( k \\right)}=\\left[ \\begin{matrix}     \\boldsymbol{A}_1&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{A}_2\\\\ \\end{matrix} \\right]\\) where \\(\\boldsymbol{A}_1\\) and \\(\\boldsymbol{A}_2\\) are tridiagonal matrices with smaller size;</li> <li>Apply QR algorithm to \\(\\boldsymbol{A}_1\\) and \\(\\boldsymbol{A}_2\\) in a recursive manner;</li> </ul> <p>End</p> <p>Two key modifications:</p> <ol> <li>Divide the big matrix into smaller matrices;</li> <li>Use the shift.</li> </ol>"},{"location":"Eigenvalue/QR_Algorithm/#about-shift","title":"About \"Shift\"","text":"<p>How can we pick the shift?</p>"},{"location":"Eigenvalue/QR_Algorithm/#method-one","title":"Method One","text":"<p>Method 1: we can pick \\(\\mu ^{\\left( k \\right)}={\\boldsymbol{A}_{m,m}}^{\\left( k \\right)}\\) as the last element of \\(\\boldsymbol{A}^k\\).</p> \\[ {\\boldsymbol{A}_{m,m}}^{\\left( k \\right)}={\\boldsymbol{e}_m}^T\\boldsymbol{A}^{\\left( k \\right)}\\boldsymbol{e}_m={\\boldsymbol{e}_m}^T\\left( \\boldsymbol{Q}^{\\left( k \\right)} \\right) ^T\\boldsymbol{AQ}^{\\left( k \\right)}\\boldsymbol{e}_m \\] \\[ =\\left( {\\boldsymbol{q}_m}^{\\left( k \\right)} \\right) ^T{\\boldsymbol{Aq}_m}^{\\left( k \\right)}=\\frac{\\left( {\\boldsymbol{q}_m}^{\\left( k \\right)} \\right) ^T{\\boldsymbol{Aq}_m}^{\\left( k \\right)}}{\\left( {\\boldsymbol{q}_m}^{\\left( k \\right)} \\right) ^T{\\boldsymbol{q}_m}^{\\left( k \\right)}} \\] <p>As \\(k\\rightarrow +\\infty\\), \\(\\boldsymbol{q}^{\\left( k \\right)}\\rightarrow \\boldsymbol{q}_m, \\mu ^{\\left( k \\right)}\\rightarrow \\lambda _m\\)</p> <p>This strategy may fail in some situations. For example, if \\(\\boldsymbol{A}=\\left[ \\begin{matrix}     0&amp;      1\\\\     1&amp;      0\\\\ \\end{matrix} \\right]\\), we get \\(\\lambda _{1,2}=\\pm 1\\). \\(\\boldsymbol{AI}=\\boldsymbol{A},\\boldsymbol{RQ}=\\boldsymbol{A}\\) remains unchanged. The shift is zero.</p>"},{"location":"Eigenvalue/QR_Algorithm/#wilkersons-shift","title":"Wilkerson's Shift","text":"<p>Method 2 (Wilkerson's Shift):</p> <p>Let:</p> \\[ \\boldsymbol{B}=\\left[ \\begin{matrix}     a_{m-1}&amp;        b_{m-1}\\\\     b_{m-1}&amp;        a_m\\\\ \\end{matrix} \\right]  \\] <p>as a lower-right 2 by 2 matrix. Pick the eigenvalue of \\(\\boldsymbol{B}\\), closer to \\(a_m\\), as the shift \\(\\mu ^{\\left( k \\right)}\\), In the case of a tie, just pick and arbitrary one:</p> \\[ \\mu ^{\\left( k \\right)}=a_m-\\mathrm{sign}\\left( \\delta \\right) \\frac{{b_{m-1}}^2}{\\left| \\delta \\right|+\\sqrt{\\delta ^2+{b_{m-1}}^2}} \\] <p>where \\(\\delta =\\frac{a_{m-1}-a_m}{2}\\). If \\(\\delta =0\\), set \\(\\delta =\\pm 1\\) arbitrarily.</p>"},{"location":"Eigenvalue/QR_Algorithm/#properties","title":"Properties","text":"<p><code>Claims</code>:</p> <ul> <li>QR algorithm with Wilkerson's Shift always converges in exact arithmetic.</li> <li>QR algorithm is backward stable.</li> <li>Overall cost of QR algorithm \\(O\\left( \\frac{4}{3}m^3 \\right)\\) flops (mainly the cost of phase 1).</li> </ul>"},{"location":"Eigenvalue/QR_Factorization/","title":"QR Factorization","text":"<p>Definition of QR Factorization:</p> \\[ \\boldsymbol{A}=\\boldsymbol{Q}\\cdot \\boldsymbol{R} \\] <p>\\(\\boldsymbol{Q}\\) is a unitary matrix and \\(\\boldsymbol{R}\\) is an upper triangular matrix.</p>"},{"location":"Eigenvalue/QR_Factorization/#gram-schmidt-algorithm","title":"Gram-Schmidt Algorithm","text":""},{"location":"Eigenvalue/QR_Factorization/#introduction-to-gram-schmidt-process","title":"Introduction to Gram-Schmidt Process","text":"<p>Gram-Schmidt is an algorithm to produce \\(\\boldsymbol{Q}\\) and \\(\\boldsymbol{R}\\).</p> <p>Given \\(\\left[ \\begin{matrix}     \\boldsymbol{a}_1&amp;       \\boldsymbol{a}_2&amp;       \\cdots&amp;     \\boldsymbol{a}_m\\\\ \\end{matrix} \\right]\\). Let \\(\\mathrm{span}\\left\\{ \\boldsymbol{a}_1, \\boldsymbol{a}_2, \\cdots , \\boldsymbol{a}_m \\right\\} \\triangleq \\left&lt; \\boldsymbol{a}_1, \\boldsymbol{a}_2, \\cdots , \\boldsymbol{a}_m \\right&gt;\\). We need to find \\(\\boldsymbol{q}_1, \\boldsymbol{q}_2, \\cdots , \\boldsymbol{q}_m\\) such that \\(\\left&lt; \\boldsymbol{q}_1, \\boldsymbol{q}_2, \\cdots , \\boldsymbol{q}_m \\right&gt; =\\left&lt; \\boldsymbol{a}_1, \\boldsymbol{a}_2, \\cdots , \\boldsymbol{a}_m \\right&gt;\\) and the inner products \\(\\left( \\boldsymbol{q}_i, \\boldsymbol{q}_j \\right) =0\\) if \\(i\\ne j\\), \\(\\left( \\boldsymbol{q}_i, \\boldsymbol{q}_i \\right) =1\\).</p> <p>Example steps (only 2-norm is used below):</p> \\[ \\boldsymbol{q}_1=\\frac{\\boldsymbol{a}_1}{\\left\\| \\boldsymbol{a}_1 \\right\\|}, r_{11}=\\left\\| \\boldsymbol{a}_1 \\right\\| \\Longrightarrow \\boldsymbol{q}_1=\\frac{\\boldsymbol{a}_1}{r_{11}} \\] \\[ r_{12}=\\left( \\boldsymbol{q}_1, \\boldsymbol{a}_2 \\right) , \\boldsymbol{v}_2=\\boldsymbol{a}_2-r_{12}\\cdot \\boldsymbol{q}_1, r_{22}=\\left\\| \\boldsymbol{v}_2 \\right\\|  \\\\ \\Longrightarrow \\boldsymbol{q}_2=\\frac{\\boldsymbol{v}_2}{r_{22}} \\] \\[ r_{13}=\\left( \\boldsymbol{q}_1, \\boldsymbol{a}_3 \\right) , r_{23}=\\left( \\boldsymbol{q}_2, \\boldsymbol{a}_3 \\right) , \\boldsymbol{v}_3=\\boldsymbol{a}_3-r_{13}\\cdot \\boldsymbol{q}_1-r_{23}\\cdot \\boldsymbol{q}_2, r_{33}=\\left\\| \\boldsymbol{v}_3 \\right\\|  \\\\ \\Longrightarrow \\boldsymbol{q}_3=\\frac{\\boldsymbol{v}_3}{r_{33}} \\]"},{"location":"Eigenvalue/QR_Factorization/#classical-gram-schmidt-algorithm","title":"Classical Gram-Schmidt Algorithm","text":"<p>Given \\(\\boldsymbol{A}=\\left[ \\begin{matrix}     \\boldsymbol{a}_1&amp;       \\cdots&amp;     \\boldsymbol{a}_j&amp;       \\cdots&amp;     \\boldsymbol{a}_m\\\\ \\end{matrix} \\right]\\);</p> <p>For \\(j=1:m\\):</p> <ul> <li>\\(\\boldsymbol{v}_j=\\boldsymbol{a}_j\\);</li> <li>For \\(i=1:(j-1)\\):<ul> <li>\\(r_{ij}=\\left( \\boldsymbol{q}_i, \\boldsymbol{a}_j \\right)\\);</li> <li>\\(\\boldsymbol{v}_j=\\boldsymbol{v}_j-r_{ij}\\cdot \\boldsymbol{q}_i\\);</li> </ul> </li> <li>End</li> <li>\\(r_{jj}=\\left\\| \\boldsymbol{v}_j \\right\\|\\);</li> <li>\\(\\boldsymbol{q}_j=\\frac{\\boldsymbol{v}_j}{r_{jj}}\\);</li> </ul> <p>End</p> <p>The outcome: \\(\\boldsymbol{Q}=\\left[ \\begin{matrix}     \\boldsymbol{q}_1&amp;       \\boldsymbol{q}_2&amp;       \\cdots&amp;     \\boldsymbol{q}_m\\\\ \\end{matrix} \\right]\\) unitary, and \\(\\boldsymbol{R}=\\left[ r_{ij} \\right]\\) upper triangular.</p> <p><code>Claim</code>: The classical Gram-Schmidt algorithm above is not stable.</p>"},{"location":"Eigenvalue/QR_Factorization/#modified-gram-schmidt-algorithm","title":"Modified Gram-Schmidt Algorithm","text":"<p>Given \\(\\boldsymbol{A}=\\left[ \\begin{matrix}     \\boldsymbol{a}_1&amp;       \\cdots&amp;     \\boldsymbol{a}_j&amp;       \\cdots&amp;     \\boldsymbol{a}_m\\\\ \\end{matrix} \\right]\\);</p> <p>For \\(i=1:m\\):</p> <ul> <li>\\(\\boldsymbol{v}_i=\\boldsymbol{a}_i\\);</li> </ul> <p>End</p> <p>For \\(i=1:m\\):</p> <ul> <li>\\(r_{ii}=\\left\\| \\boldsymbol{v}_i \\right\\|\\);</li> <li>\\(\\boldsymbol{q}_i=\\frac{\\boldsymbol{v}_i}{r_{ii}}\\);</li> <li>For \\(j=(i+1):m\\):<ul> <li>\\(r_{ij}=\\left( \\boldsymbol{q}_i, \\boldsymbol{v}_j \\right)\\);</li> <li>\\(\\boldsymbol{v}_j=\\boldsymbol{v}_j-r_{ij}\\cdot \\boldsymbol{q}_i\\);</li> </ul> </li> <li>End</li> </ul> <p>End</p> <p><code>Claim</code>: The modified Gram-Schmidt algorithm is stable.</p> <p>Questions:</p> <ol> <li>Why classical and modified Gram-Schmidt are equivalent?</li> <li>Intuitively, why is the classical one not stable, while the modified one stable?</li> </ol> <p>The cost of the modified Gram-Schmidt is \\(O(2mn^2)\\) for \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\).</p>"},{"location":"Eigenvalue/QR_Factorization/#discussion-on-qr-factorization","title":"Discussion on QR Factorization","text":"<p><code>Theorem</code>: Every matrix \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\,\\,\\left( m\\geqslant n \\right)\\) has a QR Factorization.</p> <p>If column vectors of \\(\\boldsymbol{A}\\) are linearly independent(full rank), then</p> \\[ \\left[ \\boldsymbol{A} \\right] _{m\\times n}=\\left[ \\boldsymbol{Q} \\right] _{m\\times n}\\cdot \\left[ \\boldsymbol{R} \\right] _{n\\times n} \\] <p>This is Reduced QR Factorization(default). The other one:</p> \\[ \\left[ \\boldsymbol{A} \\right] _{m\\times n}=\\underset{\\boldsymbol{Q}}{\\underbrace{\\left[ {\\boldsymbol{Q}_0}^{m\\times n}|\\cdots \\right] _{m\\times m}}}\\cdot \\underset{\\boldsymbol{R}}{\\underbrace{\\left[ \\begin{array}{c}     {\\boldsymbol{R}_0}^{n\\times n}\\\\     O\\\\ \\end{array} \\right] _{m\\times n}}} \\] <p>is called Full QR Factorization(rarely used).</p> <p><code>Theorem</code>: If \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\,\\,\\left( m\\geqslant n \\right)\\) is full rank, the Reduced QR Factorization is unique if we require \\(r_{ii}&gt;0, i=1,\\cdots ,n\\).</p> <p>We can use QR Factorization to solve linear systems (least square solution):</p> \\[ \\boldsymbol{A}=\\boldsymbol{QR} \\] \\[ \\boldsymbol{Ax}=\\boldsymbol{b}\\Leftrightarrow \\boldsymbol{QRx}=\\boldsymbol{b}\\Leftrightarrow \\boldsymbol{Rx}=\\boldsymbol{Q}^*\\boldsymbol{b} \\] <p>Matrix explanation of Gram-Schmidt process (Using upper triangular matrices to generate orthogonal matrix):</p> \\[ \\boldsymbol{A}\\cdot \\underset{\\boldsymbol{R}^{-1}}{\\underbrace{\\boldsymbol{R}_1\\boldsymbol{R}_2\\cdots \\boldsymbol{R}_n}}=\\boldsymbol{Q} \\] <p>Two types of orthogonal linear transformation:</p> <ol> <li>Rotation</li> <li>Reflection</li> </ol> <p>Actually, there are other ways to computer QR Factorization using unitary matrices to generate upper triangular matrix (Using unitary transformation):</p> \\[ \\underset{\\boldsymbol{Q}^{-1}}{\\underbrace{\\boldsymbol{Q}_n\\boldsymbol{Q}_{n-1}\\cdots \\boldsymbol{Q}_1}}\\cdot \\boldsymbol{A}=\\boldsymbol{R} \\] <p>where \\(\\boldsymbol{Q}_i\\) are unitary. There are two ways for this process: Householder transformation and Givens rotation.</p>"},{"location":"Eigenvalue/QR_Factorization/#householder-transformation","title":"Householder Transformation","text":""},{"location":"Eigenvalue/QR_Factorization/#basic-steps","title":"Basic Steps","text":"<p>The overall procedure (recursive):</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{\\boldsymbol{Q}_1}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{\\boldsymbol{Q}_2} \\] \\[ \\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{\\boldsymbol{Q}_3}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\ \\end{matrix} \\right] =\\boldsymbol{R} \\] <p>Example step 1:</p> \\[ \\boldsymbol{x}_{\\left( \\mathrm{first} \\mathrm{column} \\right)}=\\left[ \\begin{array}{c}     \\times\\\\     \\times\\\\     \\times\\\\     \\times\\\\ \\end{array} \\right] \\xrightarrow{\\boldsymbol{Q}_1}\\left[ \\begin{array}{c}     \\times\\\\     0\\\\     0\\\\     0\\\\ \\end{array} \\right]  \\] <p>where \\(\\boldsymbol{Q}_1\\) is unitary.</p> <p></p> <p>We can get:</p> \\[ \\boldsymbol{v}=-\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1-\\boldsymbol{x}, \\boldsymbol{v}\\bot \\mathbf{H} \\] \\[ \\boldsymbol{Q}_1=\\boldsymbol{I}-2\\cdot \\frac{\\boldsymbol{v}\\cdot \\boldsymbol{v}^*}{\\boldsymbol{v}^*\\boldsymbol{v}} \\] \\[ \\boldsymbol{Q}_1\\boldsymbol{x}=-\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1 \\] <p><code>Question</code>: why not choose \\(\\mathbf{H}_2\\) and get \\(\\boldsymbol{v}=\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1-\\boldsymbol{x}\\)?</p> <p>We have two choices: \\(\\boldsymbol{Q}_1\\boldsymbol{x}=-\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1, \\boldsymbol{Q}_1\\boldsymbol{x}=\\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1\\). </p> <p>In practice, we select:</p> \\[ \\boldsymbol{v}=-\\mathrm{sign}\\left( \\boldsymbol{x}_1 \\right) \\cdot \\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1-\\boldsymbol{x} \\] <p>\\(\\boldsymbol{x}_1\\) is the first entry of \\(\\boldsymbol{x}\\). We choose it for the stability reason. This results in a larger \\(\\left\\| \\boldsymbol{v} \\right\\|\\).</p> <p>Example step 2:</p> \\[ \\boldsymbol{x}_{\\left( \\mathrm{second} \\mathrm{column} \\right)}=\\left[ \\begin{array}{c}     \\times\\\\     \\times\\\\     \\times\\\\     \\times\\\\ \\end{array} \\right] \\xrightarrow{\\boldsymbol{Q}_2}\\left[ \\begin{array}{c}     \\times\\\\     \\times\\\\     0\\\\     0\\\\ \\end{array} \\right]  \\] <p>We can get:</p> \\[ \\boldsymbol{Q}_2=\\left[ \\begin{matrix}     1&amp;      0\\\\     0&amp;      \\boldsymbol{F}\\\\ \\end{matrix} \\right]  \\]"},{"location":"Eigenvalue/QR_Factorization/#householder-qr-factorization","title":"Householder QR Factorization","text":"<p>Here is the algorithm for Householder QR Factorization for a matrix \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times n}\\) :</p> <p>For \\(k=1:m\\):</p> <ul> <li>\\(\\boldsymbol{x}=\\boldsymbol{A}_{k:m,k}\\);</li> <li>\\(\\boldsymbol{v}_k=-\\mathrm{sign}\\left( x_1 \\right) \\cdot \\left\\| \\boldsymbol{x} \\right\\| \\boldsymbol{e}_1-\\boldsymbol{x}\\);</li> <li>\\(\\boldsymbol{v}_k=\\frac{\\boldsymbol{v}_k}{\\left\\| \\boldsymbol{v}_k \\right\\|}\\);</li> <li>\\(\\boldsymbol{A}_{k:m,k:n}=\\boldsymbol{A}_{k:m,k:n}-2\\boldsymbol{v}_k\\left( {\\boldsymbol{v}_k}^T\\boldsymbol{A}_{k:m,k:n} \\right)\\);</li> </ul> <p>End</p> <p>Remarks:</p> <ol> <li>QR by Householder transformation is stable. In practice, it is used more frequently than modified Gram-Schmidt algorithm.</li> <li>The cost of this algorithm is \\(O\\left( 2mn^2-\\frac{2}{3}n^3 \\right)\\).</li> <li>\\(\\boldsymbol{Q}\\) has never been formed explicitly.</li> </ol>"},{"location":"Eigenvalue/QR_Factorization/#givens-rotation-as-qr-factorization","title":"Givens Rotation as QR Factorization","text":"\\[ \\boldsymbol{A}={\\boldsymbol{T}_{12}}^*\\cdots {\\boldsymbol{T}_{1n}}^*{\\boldsymbol{T}_{23}}^*\\cdots {\\boldsymbol{T}_{2n}}^*\\cdots {\\boldsymbol{T}_{n-1,n}}^*\\boldsymbol{R}=\\boldsymbol{QR} \\] <p>The core element of Givens Rotation:</p> \\[ \\boldsymbol{T}_{pq}=\\left[ \\begin{matrix}     1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\ddots&amp;     &amp;       \\vdots&amp;     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       &amp;       \\\\     &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       \\cdots&amp;     &amp;       \\bar{c}_{\\left( p,p \\right)}&amp;       &amp;       \\cdots&amp;     &amp;       \\bar{s}_{\\left( p,q \\right)}&amp;       &amp;       \\cdots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       \\ddots&amp;     &amp;       \\vdots&amp;     &amp;       &amp;       \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       &amp;       &amp;       \\\\     &amp;       \\cdots&amp;     &amp;       -s_{\\left( q,p \\right)}&amp;        &amp;       \\cdots&amp;     &amp;       c_{\\left( q,q \\right)}&amp;     &amp;       \\cdots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1&amp;      &amp;       \\\\     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       &amp;       &amp;       \\vdots&amp;     &amp;       \\ddots&amp;     \\\\     &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       &amp;       1\\\\ \\end{matrix} \\right]  \\] \\[ c=\\cos \\theta ; \\\\ s=\\sin \\theta  \\] <p>Transformation pattern:</p> \\[ \\left[ \\begin{array}{c}     x_1\\\\     x_2\\\\     \\vdots\\\\     x_p\\\\     \\vdots\\\\     x_q\\\\     \\vdots\\\\     x_n\\\\ \\end{array} \\right] :\\left[ \\begin{array}{c}     x_p\\\\     x_q\\\\ \\end{array} \\right] \\rightarrow \\left[ \\begin{array}{c}     \\pm \\sqrt{{x_p}^2+{x_q}^2}\\\\     0\\\\ \\end{array} \\right]  \\]"},{"location":"Eigenvalue/SVD/","title":"Singular Value Decomposition (SVD)","text":""},{"location":"Eigenvalue/SVD/#svd-fundamentals","title":"SVD Fundamentals","text":""},{"location":"Eigenvalue/SVD/#unitary-matrix","title":"Unitary Matrix","text":"<p>Unitary matrix: \\(\\boldsymbol{Q}\\) is a unitary matrix if:</p> \\[ \\boldsymbol{QQ}^*=\\mathbf{I}, \\boldsymbol{Q}^*\\boldsymbol{Q}=\\mathbf{I} \\] <p>where \\(\\boldsymbol{Q}^*\\) is the conjugate transpose of \\(\\boldsymbol{Q}\\).</p> <p>A unitary matrix satisfies \\(\\left\\| \\boldsymbol{QA} \\right\\| _2=\\left\\| \\boldsymbol{A} \\right\\| _2\\), which means that a unitary transform does not change the 2-norm of a matrix. Similarly, it can also be verified that \\(\\left\\| \\boldsymbol{QA} \\right\\| _F=\\left\\| \\boldsymbol{A} \\right\\| _F\\), which means that a unitary transform does not change the Frobenius norm of a matrix.</p>"},{"location":"Eigenvalue/SVD/#introduction-to-svd","title":"Introduction to SVD","text":""},{"location":"Eigenvalue/SVD/#introductory-example","title":"Introductory Example","text":"<p><code>Example</code>: Let \\(\\boldsymbol{A}\\in \\mathbb{R} ^{2\\times 2}, \\boldsymbol{A}:\\mathbb{R} ^2\\mapsto \\mathbb{R} ^2\\). What is the image of the unit disk of \\(\\boldsymbol{A}\\) ?</p> <p></p> <p>We get:</p> \\[ \\boldsymbol{Av}_1=\\sigma _1\\boldsymbol{u}_1;\\boldsymbol{Av}_2=\\sigma _2\\boldsymbol{u}_2 \\] \\[ \\boldsymbol{A}\\left[ \\begin{matrix}     \\boldsymbol{v}_1&amp;       \\boldsymbol{v}_2\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\sigma _1\\boldsymbol{u}_1&amp;      \\sigma _2\\boldsymbol{u}_2\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{u}_1&amp;       \\boldsymbol{u}_2\\\\ \\end{matrix} \\right] \\cdot \\left[ \\begin{matrix}     \\sigma _1&amp;      0\\\\     0&amp;      \\sigma _2\\\\ \\end{matrix} \\right]  \\] <p>Let:</p> \\[ \\boldsymbol{V}=\\left[ \\begin{matrix}     \\boldsymbol{v}_1&amp;       \\boldsymbol{v}_2\\\\ \\end{matrix} \\right] , \\boldsymbol{U}=\\left[ \\begin{matrix}     \\boldsymbol{u}_1&amp;       \\boldsymbol{u}_2\\\\ \\end{matrix} \\right] , \\mathbf{\\Sigma }=\\left[ \\begin{matrix}     \\sigma _1&amp;      0\\\\     0&amp;      \\sigma _2\\\\ \\end{matrix} \\right]  \\] <p>We can get:</p> \\[ \\boldsymbol{AV}=\\boldsymbol{U}\\mathbf{\\Sigma } \\] <p>where</p> \\[ \\boldsymbol{VV}^*=\\mathbf{I}, \\boldsymbol{UU}^*=\\mathbf{I} \\] <p>Then \\(\\boldsymbol{A}=\\boldsymbol{AVV}^*=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*\\).</p>"},{"location":"Eigenvalue/SVD/#definition-and-basic-theorem","title":"Definition and Basic Theorem","text":"<p>The Singular Value Decomposition (SVD) is:</p> \\[ \\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^* \\] <p>where \\(\\boldsymbol{U}\\in \\mathbb{C} ^{m\\times m}\\) is a complex unitary matrix, \\(\\mathbf{\\Sigma }\\in \\mathbb{R} ^{m\\times n}\\) is a rectangular diagonal matrix with non-negative real numbers on the diagonal, and \\(\\boldsymbol{V}\\in \\mathbb{C} ^{n\\times n}\\) is a complex unitary matrix. (\\(\\boldsymbol{A}\\in \\mathbb{C} ^{m\\times n}\\) is a complex matrix)</p> <p>The diagonal entries \\(\\sigma _i=\\Sigma _{ii}\\) of \\(\\mathbf{\\Sigma }\\) are uniquely determined by \\(\\boldsymbol{A}\\) and are called singular values of \\(\\boldsymbol{A}\\).</p> <p><code>Theorem</code>(Existence and Uniqueness): Every matrix \\(\\boldsymbol{A}\\in \\mathbb{C} ^{m\\times n}\\) has a SVD. The singular values are uniquely determined. If \\(\\boldsymbol{A}\\) is square and \\(\\sigma _j\\) are distinct, the left and right singular vectors \\(\\boldsymbol{u}_i,\\boldsymbol{v}_i\\) are uniquely determined up to a complex sign (complex number with modular one).</p>"},{"location":"Eigenvalue/SVD/#properties-of-svd","title":"Properties of SVD","text":"<p>From:</p> \\[ \\boldsymbol{A}^*\\boldsymbol{A}=\\left( \\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^* \\right) ^*\\cdot \\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^* \\] \\[ =\\left( \\boldsymbol{V}\\mathbf{\\Sigma }^*\\boldsymbol{U}^* \\right) \\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^2\\boldsymbol{V}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^2\\boldsymbol{V}^{-1} \\] <p>We know that \\({\\sigma _i}^2\\) is the eigenvalue of \\(\\boldsymbol{A}^*\\boldsymbol{A}\\).</p> <p>We can also get \\(\\mathrm{rank}\\left( \\boldsymbol{A} \\right)\\) is the number of non-zero elements of singular values.</p> <p>Furthermore, \\(\\left\\| \\boldsymbol{A} \\right\\| _2=\\sigma _1\\) (largest singular value) if \\(\\sigma _1\\geqslant \\sigma _2\\geqslant \\cdots \\geqslant \\sigma _r\\).</p> <p>Also:</p> \\[ \\left\\| \\boldsymbol{A} \\right\\| _F=\\left( \\sum_{i=1}^r{{\\sigma _i}^2} \\right) ^{\\frac{1}{2}} \\] <p>If \\(\\boldsymbol{A}\\in \\mathbb{R} ^{m\\times m}\\), then:</p> \\[ \\left| \\det \\left( \\boldsymbol{A} \\right) \\right|=\\prod_{i=1}^m{\\sigma _i} \\]"},{"location":"Eigenvalue/SVD/#svd-computation","title":"SVD Computation","text":"<p>From:</p> \\[ \\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^* \\] <p>We can get:</p> \\[ \\boldsymbol{A}^*\\boldsymbol{A}=\\boldsymbol{V}\\underset{\\boldsymbol{D}}{\\underbrace{\\mathbf{\\Sigma }^*\\mathbf{\\Sigma }}}\\boldsymbol{V}^* \\] <p>as the eigenvalue decomposition of \\(\\boldsymbol{A}^*\\boldsymbol{A}\\). \\(\\boldsymbol{A}^*\\boldsymbol{A}\\) is called normal matrix.</p> <p>Similar to eigenvalue decomposition, SVD computation has to be iterative!</p> <p>SVD is computed in two phases as well.</p>"},{"location":"Eigenvalue/SVD/#phase-one","title":"Phase One","text":"<p>Phase 1 (Different from eigenvalue decomposition, there is no similarity requirement here): We convert \\(\\boldsymbol{A}\\) into a bidiagonal matrix.</p> <p>The algorithm is called Golub-Kahan bidiagonalization.</p> <p><code>Example</code>:</p> \\[ \\boldsymbol{A}=\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow{{\\boldsymbol{U}_1}^*}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow[\\boldsymbol{V}_1]{}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\ \\end{matrix} \\right]  \\] \\[ \\xrightarrow{{\\boldsymbol{U}_2}^*}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right] \\xrightarrow[\\boldsymbol{V}_2]{}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     0\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\ \\end{matrix} \\right]  \\] \\[ \\xrightarrow{{\\boldsymbol{U}_3}^*}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     0\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\ \\end{matrix} \\right] \\xrightarrow[\\boldsymbol{V}_3]{}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     0\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\ \\end{matrix} \\right] \\xrightarrow{{\\boldsymbol{U}_4}^*}\\left[ \\begin{matrix}     \\times&amp;     \\times&amp;     0&amp;      0\\\\     0&amp;      \\times&amp;     \\times&amp;     0\\\\     0&amp;      0&amp;      \\times&amp;     \\times\\\\     0&amp;      0&amp;      0&amp;      \\times\\\\     0&amp;      0&amp;      0&amp;      0\\\\ \\end{matrix} \\right]  \\] <p>We can get the upper bidiagonal matrix. The work for Golub-Kahan bidiagonalization is \\(O\\left( 4mn^2-\\frac{4}{3}n^3 \\right)\\) flops.</p> <p>Another method is called Lawson-Hanson-Chan algorithm. This is more efficient when \\(m\\gg n\\).</p>"},{"location":"Eigenvalue/SVD/#phase-two","title":"Phase Two","text":"<p>Phase 2: Assume \\(\\boldsymbol{A}\\) is bidiagonal. Phase 2 must be an iterative procedure. Let \\(\\sigma _k=\\sqrt{\\lambda _k}\\), where \\(\\lambda _k\\) is an eigenvalue of \\(\\boldsymbol{A}^*\\boldsymbol{A}\\). </p>"},{"location":"Eigenvalue/SVD/#regular-method","title":"Regular Method","text":"<p>A natural strategy is:</p> <ol> <li>Form \\(\\boldsymbol{A}^*\\boldsymbol{A}\\);</li> <li>Compute \\(\\boldsymbol{A}^*\\boldsymbol{A}=\\boldsymbol{V}\\mathbf{\\Lambda }\\boldsymbol{V}^*\\);</li> <li>\\(\\sqrt{\\mathbf{\\Lambda }}=\\mathbf{\\Sigma }\\) (component-wise square root);</li> <li>Determine \\(\\boldsymbol{U}=\\boldsymbol{AV}\\mathbf{\\Sigma }^{-1}\\).</li> </ol> <p>This does work well in certain cases, but not so much in other cases!</p> <p>Denote \\(\\tilde{\\sigma}_k\\) as the computed singular value: \\(\\tilde{\\sigma}_k=\\sqrt{\\tilde{\\lambda}_k}\\).</p> <p><code>Claim</code>: \\(\\left| \\tilde{\\sigma}_k-\\sigma _k \\right|=O\\left( \\frac{\\left\\| \\boldsymbol{A} \\right\\| ^2}{\\sigma _k}\\varepsilon _{\\mathrm{machine}} \\right)\\).</p> <p><code>Proof</code>:</p> <p>Bauer\u2013Fike theorem states that:</p> \\[ \\left| \\lambda _k\\left( \\boldsymbol{A}^*\\boldsymbol{A}+\\delta \\boldsymbol{B} \\right) -\\lambda _k\\left( \\boldsymbol{A}^*\\boldsymbol{A} \\right) \\right|\\leqslant \\left\\| \\delta \\boldsymbol{B} \\right\\| _2 \\] <p>Therefore, eigenvalue is a continuous function. Also \\(\\left| \\sigma _k\\left( \\boldsymbol{A}+\\delta \\boldsymbol{A} \\right) -\\sigma _k\\boldsymbol{A} \\right|\\leqslant \\left\\| \\delta \\boldsymbol{A} \\right\\| _2\\).</p> <p>If \\(\\tilde{\\lambda}_k\\) is computed by a stable method, then:</p> \\[ \\left| \\tilde{\\lambda}_k-\\lambda _k \\right|\\leqslant \\left\\| \\delta \\boldsymbol{B} \\right\\| _2; \\] \\[ \\frac{\\left\\| \\delta \\boldsymbol{B} \\right\\| _2}{\\left\\| \\boldsymbol{A}^*\\boldsymbol{A} \\right\\| _2}\\leqslant O\\left( \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>Then:</p> \\[ \\left| \\tilde{\\lambda}_k-\\lambda _k \\right|\\leqslant O\\left( \\left\\| \\boldsymbol{A}^*\\boldsymbol{A} \\right\\| _2\\cdot \\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>Take the square root:</p> \\[ \\left| \\tilde{\\sigma}_k-\\sigma _k \\right|=\\left| \\sqrt{\\tilde{\\lambda}_k}-\\sqrt{\\lambda _k} \\right|=\\left| \\frac{\\tilde{\\lambda}_k-\\lambda _k}{\\sqrt{\\tilde{\\lambda}_k}+\\sqrt{\\lambda _k}} \\right|=O\\left( \\frac{\\left| \\tilde{\\lambda}_k-\\lambda _k \\right|}{2\\sqrt{\\lambda _k}} \\right)  \\] \\[ \\leqslant O\\left( \\frac{\\left\\| \\boldsymbol{A}^*\\boldsymbol{A} \\right\\| _2\\varepsilon _{\\mathrm{machine}}}{2\\sigma _k} \\right) \\approx O\\left( \\frac{{\\left\\| \\boldsymbol{A} \\right\\| _2}^2}{\\sigma _k}\\varepsilon _{\\mathrm{machine}} \\right)  \\] <p>We end the proof.</p> <p>Note: If \\(\\sigma _k\\sim {\\left\\| \\boldsymbol{A} \\right\\| _2}^2\\), the computation is accurate. But if \\(\\sigma _k\\ll {\\left\\| \\boldsymbol{A} \\right\\| _2}^2\\), this is not a good strategy.</p>"},{"location":"Eigenvalue/SVD/#better-method","title":"Better Method","text":"<p>If \\(\\frac{{\\left\\| \\boldsymbol{A} \\right\\| _2}^2}{\\sigma _k}\\) is large, is there a better way to compute the SVD is phase 2? The answer is yes.</p> <p>Form a matrix:</p> \\[ \\boldsymbol{H}=\\left[ \\begin{matrix}     \\boldsymbol{O}&amp;     \\boldsymbol{A}^*\\\\     \\boldsymbol{A}&amp;     \\boldsymbol{O}\\\\ \\end{matrix} \\right]  \\] <p>Note that \\(\\boldsymbol{A}=\\boldsymbol{U}\\mathbf{\\Sigma }\\boldsymbol{V}^*, \\boldsymbol{A}^*=\\boldsymbol{V}\\mathbf{\\Sigma }^*\\boldsymbol{U}^*\\). We can verify:</p> \\[ \\left[ \\begin{matrix}     \\boldsymbol{O}&amp;     \\boldsymbol{A}^*\\\\     \\boldsymbol{A}&amp;     \\boldsymbol{O}\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix}     \\boldsymbol{V}&amp;     \\boldsymbol{V}\\\\     \\boldsymbol{U}&amp;     -\\boldsymbol{U}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{V}&amp;     \\boldsymbol{V}\\\\     \\boldsymbol{U}&amp;     -\\boldsymbol{U}\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix}     \\mathbf{\\Sigma }&amp;       \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     -\\mathbf{\\Sigma }\\\\ \\end{matrix} \\right] ; \\] \\[ \\left[ \\begin{matrix}     \\boldsymbol{V}^*&amp;       \\boldsymbol{U}^*\\\\     \\boldsymbol{V}^*&amp;       -\\boldsymbol{U}^*\\\\ \\end{matrix} \\right] \\left[ \\begin{matrix}     \\boldsymbol{V}&amp;     \\boldsymbol{V}\\\\     \\boldsymbol{U}&amp;     -\\boldsymbol{U}\\\\ \\end{matrix} \\right] =\\left[ \\begin{matrix}     \\boldsymbol{V}^*\\boldsymbol{V}+\\boldsymbol{U}^*\\boldsymbol{U}&amp;      \\boldsymbol{O}\\\\     \\boldsymbol{O}&amp;     \\boldsymbol{V}^*\\boldsymbol{V}+\\boldsymbol{U}^*\\boldsymbol{U}\\\\ \\end{matrix} \\right]  \\] <p>This helps with the eigenvalue decomposition of \\(\\boldsymbol{H}\\). The singular values of \\(\\boldsymbol{A}\\) are the absolute values of the eigenvalues of \\(\\boldsymbol{H}\\). Therefore, the eigenvalues of \\(\\boldsymbol{H}\\) give the singular values of \\(\\boldsymbol{A}\\).</p>"},{"location":"def/Eigenvalue_and_Eigenvector/","title":"Eigen Value and Vecotr","text":"<p>Definition : Consider a square matrix \\(A=[a_{ij}]_{n{\\times}n}\\) and a nonzero vector \\(v\\) of length \\(n\\) Then there exit a scalar \\(\\lambda \\in R\\) such that \\(Av = \\lambda v\\)  . Where \\(v\\) is called eigenvector corresponding to eigenvalue \\(\\lambda\\) of matrix \\(A\\).</p> <p>Remark :  1. Linear transformation \\(T : R^n \\to R^n\\) is equivalent to the square matrix \\(A\\) of order \\(n \\times n\\). thus given a basis set of the vector space can be defined as set of eigen vectors of matrix \\(A\\) for linear transformation \\(T\\).</p> <ol> <li>Eigenvectors and eigenvalues exits in a wide range of applications  like stability and vibration analysis of dynamical systems, atomic orbits, facial recognition and matrix diagonalization.</li> </ol>"},{"location":"def/Eigenvalue_and_Eigenvector/#faddeev-leverrier-algorithm","title":"Faddeev-LeVerrier Algorithm","text":"<p>Faddeev-LeVerrier algorithm is a recursive method to calculate the coefficients of the characteristic polynomial  \\(p_A(\\lambda)=\\det (\\lambda I_n - A)\\)  of a square matrix, A. solving characteristic polynomial gives eigen values of matrix A as a roots of it and matrix polynomial of matrix A  vanishes i.e p(A) = 0 by Cayley-Hamilton Theorem. Faddeev-Le Verrier algorithm works directly with coefficients of matrix \\(A\\).</p> <p>Problem : Given a </p>"},{"location":"def/Linear_Transformations/","title":"Linear Transformations","text":"<p>Given two linear space V and W over a field F, a linear map \\(T:V\\to W\\) that preserve addition and scalar multiplication such that</p> \\[ \\begin{equation} T(\\mathbf u + \\mathbf v)=T(\\mathbf u)+T(\\mathbf v)  \\\\  \\quad T(a \\mathbf v)=aT(\\mathbf v) \\end{equation} \\] <p>or we can write in general via linear combination as</p> \\[ \\\\ \\; \\begin{equation} T(a \\mathbf u + b \\mathbf v)= T(a \\mathbf u) + T(b \\mathbf v) = aT(\\mathbf u) + bT(\\mathbf v)  \\end{equation} \\] <p>for all \\(u, v \\in V\\) and scaler \\(a, b \\in F\\)</p> <p>remark-1 : If \\(V = W\\) are the same linear space then linear map \\(T\u00a0: V \u2192 V\\) is also known as a linear operator on \\(V\\).</p> <p>remark-2 : A bijective linear map between two linear spaces  is an isomorphism because it preserves linear structure and two isomorphic linear spaces are same algebraically means we can't make any distiction between these two spaces using linear space properties.</p> <p>remark-3 : How to check wheather a linear map is isomorphic or not. If it is non-isomorphic then we find its range space(set of elements which have non-zero images) and null space(set of elements which have zero images also called kernel of T).</p>"}]}